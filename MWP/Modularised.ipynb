{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy==2.1.0\n",
        "!python -m spacy download en\n",
        "\n",
        "!pip install Cython --install-option=\"â€“no-cython-compile\"\n",
        "\n",
        "!pip install neuralcoref\n",
        "\n",
        "!pip install benepar\n",
        "\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "!spacy download en"
      ],
      "metadata": {
        "id": "pwopgc-5MJou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGxfkXUzKm9m",
        "outputId": "9900fc78-e8d3-4231-c9be-1886177b0628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "import benepar\n",
        "import neuralcoref\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "class MicroStatments:\n",
        "  \"\"\"\n",
        "  return microstatemnts from input mwp\n",
        "  coref resolved and conjunction resolved.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.nlp_pronoun = spacy.load('en')\n",
        "    neuralcoref.add_to_pipe(self.nlp_pronoun, greedyness=0.52)\n",
        "    benepar.download('benepar_en3')\n",
        "    self.nlp = spacy.load('en_core_web_md')\n",
        "    if spacy.__version__.startswith('2'):\n",
        "        self.nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
        "    else:\n",
        "        self.nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "  def __replace(self, sentence):\n",
        "    doc = self.nlp_pronoun(sentence)\n",
        "    return doc._.coref_resolved\n",
        "\n",
        "  def __extract_sub_verb_object(self, sentence):\n",
        "    subject = \"\"\n",
        "    verb_phrase = \"\"\n",
        "    preposition_phrase = \"\"\n",
        "    #dependency_tree = sent1._.parse_string\n",
        "    doc = self.nlp(sentence)\n",
        "    sent = list(doc.sents)[0]\n",
        "    dependency_tree = sent._.parse_string\n",
        "    sb = 1\n",
        "    vb = 0\n",
        "    pb = 0\n",
        "\n",
        "    for i in range(len(dependency_tree)):\n",
        "      ch = dependency_tree[i]\n",
        "\n",
        "      if sb == 1:\n",
        "        if (ch.islower()==True or ch.isdigit()==True or ch==\" \" or ch==\",\" ):\n",
        "          if( (subject[:-1]!=\" \" and ch==\" \") or ch!=\" \"):\n",
        "              subject = subject+ch\n",
        "        elif ch==\"V\":\n",
        "          sb = 0\n",
        "          vb = 1\n",
        "          continue\n",
        "\n",
        "      if vb == 1:\n",
        "        if (ch.islower()==True or ch.isdigit()==True or ch==\" \" or ch==\",\"):\n",
        "          if( (verb_phrase[:-1]!=\" \" and ch==\" \") or ch!=\" \"):\n",
        "            verb_phrase = verb_phrase+ch\n",
        "          \n",
        "        elif ch==\"N\" and dependency_tree[i+1] == \"P\":\n",
        "          vb = 0\n",
        "          pb = 1\n",
        "          continue\n",
        "\n",
        "      if pb == 1:\n",
        "          if (ch.islower()==True or ch.isdigit()==True or ch==\" \" or ch==\",\"):\n",
        "            if( (preposition_phrase[:-1]!=\" \" and ch==\" \") or ch!=\" \"):\n",
        "              preposition_phrase = preposition_phrase+ch\n",
        "      \n",
        "    return {\"subject\":subject.strip(), \"verb\":verb_phrase.strip(), \"object\":preposition_phrase.strip()}\n",
        "\n",
        "\n",
        "  def __split_conj(self, sent_list):\n",
        "    sub = \"\"\n",
        "    l = list()\n",
        "\n",
        "    for i in sent_list:\n",
        "      if i!=\"and\":\n",
        "          sub = sub+\" \"+i\n",
        "      else:\n",
        "        if sub!=\"\":\n",
        "          l.append(sub)\n",
        "          sub = \"\"\n",
        "\n",
        "    l.append(sub)\n",
        "    return l\n",
        "  \n",
        "  def __handle_conjunction(self, sentence):\n",
        "    d = self.__extract_sub_verb_object(sentence)\n",
        "\n",
        "    subject = d[\"subject\"].replace(\",\",\"and\")\n",
        "    verb = d[\"verb\"].replace(\",\",\"and\")\n",
        "    obj = d[\"object\"].replace(\",\",\"and\")\n",
        "    subject_tokens = [i for i in subject.split(' ') if i != ' ']\n",
        "    verb_tokens = [i for i in verb.split(' ') if i!= ' ']\n",
        "    obj_tokens = [i for i in obj.split(' ') if i!=''] \n",
        "    sentences = list()\n",
        "    if \"and\" in subject_tokens:\n",
        "      split_subject = self.__split_conj(subject_tokens)\n",
        "      \n",
        "      for i in split_subject:\n",
        "        sentences.append(i+\" \"+verb+\" \"+obj)\n",
        "      return sentences\n",
        "    if \"and\" in obj_tokens:\n",
        "      \n",
        "      split_obj = self.__split_conj(obj_tokens)\n",
        "    \n",
        "      \n",
        "      for i in split_obj:\n",
        "        sentences.append(subject+\" \" + verb+\" \"+ i)\n",
        "        \n",
        "      return sentences\n",
        "    else:\n",
        "      sentences.append(subject+\" \"+verb+\" \"+obj)\n",
        "      return sentences\n",
        "    \n",
        "  def mwp_split(self, mwp):\n",
        "    mwp_split_temp = sent_tokenize(mwp)\n",
        "    mwp_split = list()\n",
        "    res = []\n",
        "    for i in mwp_split_temp:\n",
        "      temp = self.__handle_conjunction(i)\n",
        "      for j in temp:\n",
        "        mwp_split.append(j.strip())\n",
        "    for i, sent in enumerate(mwp_split):\n",
        "      if \"and\" in sent:\n",
        "        res.extend(self.__handle_conjunction(sent))\n",
        "        continue\n",
        "      res.append(sent)\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ms = MicroStatments()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf9aJ-9GP3hL",
        "outputId": "83195f9d-ac25-4f3a-fe3b-baf8c0691001"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ms.mwp_split(\"john and rebecca had 2 apples and 2 oranges\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvQr5BsUQBKi",
        "outputId": "3954fe20-741f-40ff-ec68-20d96da69a25"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['john had  2 apples',\n",
              " 'john had  2 oranges',\n",
              " 'rebecca had  2 apples',\n",
              " 'rebecca had  2 oranges']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4zZhW5mV69x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}