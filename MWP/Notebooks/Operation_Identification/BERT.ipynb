{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e92fb4",
   "metadata": {},
   "source": [
    "# Operation Identification using Deep Learning (BERT)\n",
    "##### Since the accuracy on unseen data was previously low(48%), I'm going to combine both the datasets(singleop.json and SVAMP.json) and see if it gets any better!\n",
    "\n",
    "1. Make all samples equally probable\n",
    "2. Just join and see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9797a85",
   "metadata": {},
   "source": [
    "# 1 Import and Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd9029ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0551dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "so = pd.read_csv('cleaned_singleop.csv')\n",
    "sv = pd.read_csv('cleaned_svamp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f139014f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Addition</td>\n",
       "      <td>if there are bottle caps in a box and linda pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Addition</td>\n",
       "      <td>jose starts with bottle caps he gets more from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addition</td>\n",
       "      <td>bridget has skittles henry has skittles if hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addition</td>\n",
       "      <td>brenda starts with skittles she buys more how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Addition</td>\n",
       "      <td>there are cards cards more are added how many ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type                                              Clean\n",
       "0  Addition  if there are bottle caps in a box and linda pu...\n",
       "1  Addition  jose starts with bottle caps he gets more from...\n",
       "2  Addition  bridget has skittles henry has skittles if hen...\n",
       "3  Addition  brenda starts with skittles she buys more how ...\n",
       "4  Addition  there are cards cards more are added how many ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "so = so[['Type', 'Clean']]\n",
    "so.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b319df37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = sv[['Type', 'Clean']]\n",
    "len(sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f379e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Multiplication</td>\n",
       "      <td>a bee has legs how many legs do bees have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Multiplication</td>\n",
       "      <td>a bee has legs how many legs do bees have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Subtraction</td>\n",
       "      <td>a book has chapters across pages the first cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>Subtraction</td>\n",
       "      <td>a book has chapters across pages the second ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Multiplication</td>\n",
       "      <td>a book has chapters each chapter is pages long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Subtraction</td>\n",
       "      <td>zachary did pushups and david did pushups in g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Addition</td>\n",
       "      <td>zachary did pushups in gym class today david d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>Subtraction</td>\n",
       "      <td>zachary did pushups in gym class today david d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>Addition</td>\n",
       "      <td>zachary did pushups in gym class today david d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Subtraction</td>\n",
       "      <td>zachary did pushups in gym class today david d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1562 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Type                                              Clean\n",
       "398  Multiplication          a bee has legs how many legs do bees have\n",
       "295  Multiplication          a bee has legs how many legs do bees have\n",
       "729     Subtraction  a book has chapters across pages the first cha...\n",
       "717     Subtraction  a book has chapters across pages the second ch...\n",
       "445  Multiplication  a book has chapters each chapter is pages long...\n",
       "..              ...                                                ...\n",
       "360     Subtraction  zachary did pushups and david did pushups in g...\n",
       "365        Addition  zachary did pushups in gym class today david d...\n",
       "469     Subtraction  zachary did pushups in gym class today david d...\n",
       "460        Addition  zachary did pushups in gym class today david d...\n",
       "239     Subtraction  zachary did pushups in gym class today david d...\n",
       "\n",
       "[1562 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([sv, so])\n",
    "df = df.sort_values('Clean')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef76d091",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Type.value_counts().sort_values().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a286e",
   "metadata": {},
   "source": [
    "# 2. Build Model\n",
    "4 classes are ['Addition','Division', 'Multiplication', 'Subtraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88eeaa79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163a970",
   "metadata": {},
   "source": [
    "## 2.1 Encode the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60820f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Multiplication': 0, 'Subtraction': 1, 'Addition': 2, 'Division': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.Type.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8894c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.Type.replace(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454ac68",
   "metadata": {},
   "source": [
    "## 2.2 Train & Validation Split\n",
    "#### Since the data is very imbalanced, I'm splitting the dataset into a stratified way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d63b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d885e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values,\n",
    "                                                 df.label.values,\n",
    "                                                 test_size=0.15,\n",
    "                                                 random_state=42,\n",
    "                                                 stratify=df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fab848e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Addition</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Division</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Multiplication</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Subtraction</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Clean\n",
       "Type           label data_type       \n",
       "Addition       2     train        267\n",
       "                     val           87\n",
       "Division       3     train        218\n",
       "                     val           72\n",
       "Multiplication 0     train        167\n",
       "                     val           58\n",
       "Subtraction    1     train        531\n",
       "                     val          162"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many of each were selected\n",
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['Type', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a5f63",
   "metadata": {},
   "source": [
    "## 2.3 BERT TOKENIZER and ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089adc9c",
   "metadata": {},
   "source": [
    "- Constructs a BERT tokenizer. Based on WordPiece.\n",
    "- Instantiate a pre-trained BERT model configuration to encode our data.\n",
    "- To convert all the titles from text into encoded form, we use a function called batch_encode_plus , and we will proceed train and validation data separately.\n",
    "- The 1st parameter inside the above function is the title text.\n",
    "- add_special_tokens=True means the sequences will be encoded with the special tokens relative to their model.\n",
    "- When batching sequences together, we set return_attention_mask=True, so it will return the attention mask according to the specific tokenizer defined by the max_length attribute.\n",
    "- We also want to pad all the titles to certain maximum length.\n",
    "- We actually do not need to set max_length=256, but just to play it safe.\n",
    "- return_tensors='pt' to return PyTorch.\n",
    "- And then we need to split the data into input_ids, attention_masks and labels.\n",
    "- Finally, after we get encoded data set, we can create training data and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9783d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\d3583\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2300: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].Clean.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].Clean.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# train\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
    "\n",
    "# validation\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce577f52",
   "metadata": {},
   "source": [
    "## 2.4 BERT Pre-trained Model\n",
    "Treat each title as unique seq, so one seq is classified to one of the labels (Operations)\n",
    "- bert-base-uncased is smaller pre-trained model (still huge, 440MB omg)\n",
    "- num_labels = # output labels\n",
    "- don't care about output_attentions & output_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8965b72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c2afc",
   "metadata": {},
   "source": [
    "## 2.5 Data Loaders\n",
    "Makes an iterable over given dataset by combining dataset and sampler.\n",
    "RandomSampler - training\n",
    "SequentialSampler - for validation\n",
    "\n",
    "adjust batch_size based on memory available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6dd73f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e461fe",
   "metadata": {},
   "source": [
    "## 2.6 Optimizer & Scheduler\n",
    "To construct an optimizer, we have to give it an iterable containing the parameters to optimize. Then, we can specify optimizer-specific options such as the learning rate, epsilon, etc.\n",
    "I found epochs=5 works well for this data set.\n",
    "Create a schedule with a learning rate that decreases linearly from the initial learning rate set in the optimizer to 0, after a warmup period during which it increases linearly from 0 to the initial learning rate set in the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b38c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 2\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02eeafd",
   "metadata": {},
   "source": [
    "## 2.7 Performance Metrics\n",
    "F1 & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bbd9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}')\n",
    "        acc = (len(y_preds[y_preds==label])/len(y_true))*100\n",
    "        print(f'Accuracy in %: {acc}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d53416e",
   "metadata": {},
   "source": [
    "# 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82008f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90fc6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55d1915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "322c2f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f02ab7d64c4062abaa9a8171377520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.6815182261829135\n",
      "Validation loss: 0.5796847866158786\n",
      "F1 Score (Weighted): 0.7793015584486337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.46514586487997184\n",
      "Validation loss: 0.484202607952821\n",
      "F1 Score (Weighted): 0.8314800440854269\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'ep2finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8973f",
   "metadata": {},
   "source": [
    "## 4. Load and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5d6551b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26acd276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Multiplication\n",
      "Accuracy: 41/58\n",
      "Accuracy in %: 70.6896551724138\n",
      "\n",
      "Class: Subtraction\n",
      "Accuracy: 143/162\n",
      "Accuracy in %: 88.27160493827161\n",
      "\n",
      "Class: Addition\n",
      "Accuracy: 70/87\n",
      "Accuracy in %: 80.45977011494253\n",
      "\n",
      "Class: Division\n",
      "Accuracy: 61/72\n",
      "Accuracy in %: 84.72222222222221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('ep2finetuned_BERT_epoch_2.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
