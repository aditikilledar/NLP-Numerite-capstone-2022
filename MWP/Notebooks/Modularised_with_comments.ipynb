{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy==2.1.0\n",
        "!python -m spacy download en\n",
        "\n",
        "!pip install Cython --install-option=\"–no-cython-compile\"\n",
        "\n",
        "!pip install neuralcoref\n",
        "\n",
        "!pip install benepar\n",
        "\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "!spacy download en"
      ],
      "metadata": {
        "id": "pwopgc-5MJou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60acbb81-3bc1-430f-b36b-b70bf22b3ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.1.0\n",
            "  Downloading spacy-2.1.0-cp37-cp37m-manylinux1_x86_64.whl (27.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7 MB 23.4 MB/s \n",
            "\u001b[?25hCollecting thinc<7.1.0,>=7.0.2\n",
            "  Downloading thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.0.6)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "  Downloading preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 359 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.8)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "  Downloading blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 42.5 MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
            "Collecting jsonschema<3.0.0,>=2.6.0\n",
            "  Downloading jsonschema-2.6.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting srsly<1.1.0,>=0.0.5\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 45.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.64.1)\n",
            "Installing collected packages: srsly, preshed, plac, blis, thinc, jsonschema, spacy\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.7\n",
            "    Uninstalling preshed-3.0.7:\n",
            "      Successfully uninstalled preshed-3.0.7\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.3.3\n",
            "    Uninstalling jsonschema-4.3.3:\n",
            "      Successfully uninstalled jsonschema-4.3.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.1.0 which is incompatible.\n",
            "altair 4.2.0 requires jsonschema>=3.0, but you have jsonschema 2.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.2.4 jsonschema-2.6.0 plac-0.9.6 preshed-2.0.1 spacy-2.1.0 srsly-1.0.5 thinc-7.0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_sm==2.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 2.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-py3-none-any.whl size=11074433 sha256=e02ac26f0d3401fd2d78cb612790593d320d37660497e30b1fa33626cf79585c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b0f1do4i/wheels/59/4f/8c/0dbaab09a776d1fa3740e9465078bfd903cc22f3985382b496\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.4.0\n",
            "    Uninstalling en-core-web-sm-3.4.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.4.0\n",
            "Successfully installed en-core-web-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.32)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neuralcoref\n",
            "  Downloading neuralcoref-4.0-cp37-cp37m-manylinux1_x86_64.whl (286 kB)\n",
            "\u001b[K     |████████████████████████████████| 286 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.23.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.24.75-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.8)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.10.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.64.1)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting botocore<1.28.0,>=1.27.75\n",
            "  Downloading botocore-1.27.75-py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 43.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.75->boto3->neuralcoref) (2.8.2)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.75->boto3->neuralcoref) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.24.75 botocore-1.27.75 jmespath-1.0.1 neuralcoref-4.0 s3transfer-0.6.0 urllib3-1.25.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting benepar\n",
            "  Downloading benepar-0.2.0.tar.gz (33 kB)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.7)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar) (1.12.1+cu113)\n",
            "Collecting torch-struct>=0.5\n",
            "  Downloading torch_struct-0.5-py3-none-any.whl (34 kB)\n",
            "Collecting tokenizers>=0.9.4\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 9.8 MB/s \n",
            "\u001b[?25hCollecting transformers[tokenizers,torch]>=4.2.2\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar) (3.17.3)\n",
            "Collecting sentencepiece>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (7.1.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.21.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.2.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.23.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (7.0.8)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->benepar) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.12.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[tokenizers,torch]>=4.2.2->benepar) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[tokenizers,torch]>=4.2.2->benepar) (3.8.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->benepar) (1.15.0)\n",
            "Building wheels for collected packages: benepar\n",
            "  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37647 sha256=049c24a0e09b5420461e7b82ba78f12b0c825283a316ce85c7d2ac8ca4ca6616\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/6f/a3/4d27ce92766bdedd2cbbbedb8857fb7a53534331191cda4994\n",
            "Successfully built benepar\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, torch-struct, sentencepiece, benepar\n",
            "Successfully installed benepar-0.2.0 huggingface-hub-0.9.1 sentencepiece-0.1.97 tokenizers-0.12.1 torch-struct-0.5 transformers-4.22.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_md==2.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4 MB 1.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-py3-none-any.whl size=97126237 sha256=d9f5b01c62570f50cce738712df6b78764b67ec1aa49fb96ce638b4cbec6c070\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n0o_qqrl/wheels/1a/4e/53/ca2bd8efb94658d2425a0a2d998ffddc2db7ea1378421f8565\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_sm==2.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 5.1 MB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGxfkXUzKm9m",
        "outputId": "e2fcebd7-eca7-48f4-952c-2698bf9e529a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "import benepar\n",
        "import neuralcoref\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "class MicroStatments:\n",
        "  \"\"\"\n",
        "  return microstatemnts from input mwp\n",
        "  coref resolved and conjunction resolved.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    initialising the neuralcoref pipeline\n",
        "    \"\"\"\n",
        "    self.nlp_pronoun = spacy.load('en')\n",
        "    neuralcoref.add_to_pipe(self.nlp_pronoun, greedyness=0.52)\n",
        "    benepar.download('benepar_en3')\n",
        "    self.nlp = spacy.load('en_core_web_md')\n",
        "    if spacy.__version__.startswith('2'):\n",
        "        self.nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
        "    else:\n",
        "        self.nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "  def replace(self, sentence):\n",
        "    \"\"\" \n",
        "    Neural coref resolution function\n",
        "    @input : sentence without coref resolution\n",
        "    @output : coref resolved sentence \n",
        "    \"\"\"\n",
        "    doc = self.nlp_pronoun(sentence)\n",
        "    return doc._.coref_resolved\n",
        "\n",
        "  def __extract_sub_verb_object(self, sentence):\n",
        "    \"\"\"\n",
        "    @input : sentence with coref resolution\n",
        "    @output : {sub: subject of sentence, \n",
        "                verb: verb of sentence, \n",
        "                object : object of sentence}\n",
        "    \n",
        "    Performed using Parsing a dependenct tree generated by benepar (berkeley neural parser)\n",
        "    \"\"\"\n",
        "\n",
        "    # initialise empty strings for S, V and O\n",
        "    subject = \"\"\n",
        "    verb_phrase = \"\"\n",
        "    preposition_phrase = \"\"\n",
        "    #dependency_tree = sent1._.parse_string\n",
        "    doc = self.nlp(sentence)\n",
        "    sent = list(doc.sents)[0]\n",
        "    dependency_tree = sent._.parse_string\n",
        "\n",
        "    # 3 flags to keep track of which part of sentence we're in\n",
        "    # assume we always start with subject -> sb = 1\n",
        "    sb = 1\n",
        "    vb = 0\n",
        "    pb = 0\n",
        "\n",
        "    for i in range(len(dependency_tree)):\n",
        "      ch = dependency_tree[i]\n",
        "\n",
        "      if sb == 1: # sub part\n",
        "        if (ch.islower()==True or ch.isdigit()==True or ch==\" \" or ch==\",\" ):\n",
        "          if( (subject[:-1]!=\" \" and ch==\" \") or ch!=\" \"):\n",
        "              subject = subject+ch\n",
        "\n",
        "        elif ch==\"V\": # verb part, set vb = 1\n",
        "          sb = 0\n",
        "          vb = 1\n",
        "          continue\n",
        "\n",
        "      if vb == 1:\n",
        "        if (ch.islower()==True or ch.isdigit()==True or ch==\" \" or ch==\",\"):\n",
        "          if( (verb_phrase[:-1]!=\" \" and ch==\" \") or ch!=\" \"):\n",
        "            verb_phrase = verb_phrase+ch\n",
        "          \n",
        "        elif ch==\"N\" and dependency_tree[i+1] == \"P\": # obj part, set pb = 1\n",
        "          vb = 0\n",
        "          pb = 1\n",
        "          continue\n",
        "\n",
        "      if pb == 1:\n",
        "          if (ch.islower()==True or ch.isdigit()==True or ch==\" \" or ch==\",\"):\n",
        "            if( (preposition_phrase[:-1]!=\" \" and ch==\" \") or ch!=\" \"):\n",
        "              preposition_phrase = preposition_phrase+ch\n",
        "      \n",
        "    return {\"subject\":subject.strip(), \"verb\":verb_phrase.strip(), \"object\":preposition_phrase.strip()}\n",
        "\n",
        "\n",
        "  def __split_conj(self, sent_list):\n",
        "    \"\"\"\n",
        "    split conjunctions at \"and\"\n",
        "\n",
        "    \"\"\"\n",
        "    sub = \"\"\n",
        "    l = list()\n",
        "\n",
        "    for i in sent_list:\n",
        "      if i!=\"and\":\n",
        "          sub = sub+\" \"+i\n",
        "      else:\n",
        "        if sub!=\"\":\n",
        "          l.append(sub)\n",
        "          sub = \"\"\n",
        "\n",
        "    l.append(sub)\n",
        "    return l\n",
        "  \n",
        "  def __handle_conjunction(self, sentence):\n",
        "    d = self.__extract_sub_verb_object(sentence)\n",
        "\n",
        "    subject = d[\"subject\"].replace(\",\",\"and\")\n",
        "    verb = d[\"verb\"].replace(\",\",\"and\")\n",
        "    obj = d[\"object\"].replace(\",\",\"and\")\n",
        "    subject_tokens = [i for i in subject.split(' ') if i != ' ']\n",
        "    verb_tokens = [i for i in verb.split(' ') if i!= ' ']\n",
        "    obj_tokens = [i for i in obj.split(' ') if i!=''] \n",
        "    sentences = list()\n",
        "    if \"and\" in subject_tokens:\n",
        "      split_subject = self.__split_conj(subject_tokens)\n",
        "      \n",
        "      for i in split_subject:\n",
        "        sentences.append(i+\" \"+verb+\" \"+obj)\n",
        "      return sentences\n",
        "    if \"and\" in obj_tokens:\n",
        "      \n",
        "      split_obj = self.__split_conj(obj_tokens)\n",
        "    \n",
        "      \n",
        "      for i in split_obj:\n",
        "        sentences.append(subject+\" \" + verb+\" \"+ i)\n",
        "        \n",
        "      return sentences\n",
        "    else:\n",
        "      sentences.append(subject+\" \"+verb+\" \"+obj)\n",
        "      return sentences\n",
        "    \n",
        "  def mwp_split(self, mwp):\n",
        "    mwp_split_temp = sent_tokenize(mwp)\n",
        "    mwp_split = list()\n",
        "    res = []\n",
        "    \n",
        "    for i in mwp_split_temp:\n",
        "      temp = self.__handle_conjunction(i)\n",
        "      for j in temp:\n",
        "        mwp_split.append(j.strip())\n",
        "    \n",
        "    for i, sent in enumerate(mwp_split):\n",
        "      if \"and\" in sent:\n",
        "        res.extend(self.__handle_conjunction(sent))\n",
        "        continue\n",
        "      res.append(sent)\n",
        "    \n",
        "    res = self.__keep_relevant(res)\n",
        "    \n",
        "    return res\n",
        "\n",
        "  def statements(self, mwp):\n",
        "    mwp_split_temp = sent_tokenize(mwp)\n",
        "    mwp_split = list()\n",
        "    res = []\n",
        "    \n",
        "    for i in mwp_split_temp:\n",
        "      temp = self.__handle_conjunction(i)\n",
        "      for j in temp:\n",
        "        mwp_split.append(j.strip())\n",
        "    \n",
        "    for i, sent in enumerate(mwp_split):\n",
        "      if \"and\" in sent:\n",
        "        res.extend(self.__handle_conjunction(sent))\n",
        "        continue\n",
        "      res.append(sent)\n",
        "    \n",
        "    res = self.keep_relevant(res)\n",
        "    \n",
        "    return res\n",
        "\n",
        "  def __keep_relevant(self, microsents):\n",
        "    \"\"\"\n",
        "    extracts all nouns from question -> store in a set (qnouns)\n",
        "    compare nouns of question (qnouns) with set of nouns in each microstatement\n",
        "    keep only those microstatements which have all nouns in qnouns   \n",
        "\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    is_noun = lambda pos: pos[:2] == 'NN'\n",
        "    \n",
        "    question = microsents[-1]\n",
        "    qtokens = nltk.word_tokenize(question)\n",
        "    #print(qtokens)\n",
        "    #print(nltk.pos_tag(qtokens))\n",
        "    qnouns = {word for (word, pos) in nltk.pos_tag(qtokens) if is_noun(pos)}\n",
        "    print(qnouns)\n",
        "    for sent in microsents:\n",
        "      tokenized = nltk.word_tokenize(sent)\n",
        "      nouns = {word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)}\n",
        "      if len(qnouns.difference(nouns)) == 0:\n",
        "        res.append(sent)\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ms.keep_relevant(['mary had a basket which has 12 peaches'])"
      ],
      "metadata": {
        "id": "18WWQkQbwyIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8798889-35d2-4e90-b3ad-417c625dc574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'peaches', 'mary', 'basket'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mary had a basket which has 12 peaches']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ms = MicroStatments()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf9aJ-9GP3hL",
        "outputId": "b4ae3333-c370-4afd-d7f8-0c7b102554f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ms.mwp_split(\"john and rebecca had 2 apples and 2 oranges. how many oranges do rebecca and john both have\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvQr5BsUQBKi",
        "outputId": "d83ade2a-eb78-4fe4-ec32-f1bc7e8d0d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['john had  2 oranges',\n",
              " 'rebecca had  2 oranges',\n",
              " 'how  many  oranges do  rebecca',\n",
              " 'how  many  oranges do  john both have']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ms.statements(\"john and rebecca had 2 apples and 2 oranges. john then handed rebecca 2 apples. how many apples does rebecca have?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4zZhW5mV69x",
        "outputId": "144afc65-f501-4582-ec4b-f6bcfac43c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  'with `validate_args=False` to turn off validation.')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rebecca had  2 apples',\n",
              " 'john then         handed  rebecca      2     apples ',\n",
              " 'how  many  apples does rebecca   have']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_pronoun = spacy.load('en')\n",
        "import neuralcoref\n",
        "neuralcoref.add_to_pipe(nlp_pronoun,greedyness=0.52)\n",
        "\n",
        "def replace(sentence):\n",
        "  doc = nlp_pronoun(sentence)\n",
        "  return doc._.coref_resolved"
      ],
      "metadata": {
        "id": "S5tXdis-r79o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json('SVAMP.json')\n",
        "#df['Body'] = [replace(df['Body'][i] + ' ' + df['Question'][i]) for i in range(len(df))]"
      ],
      "metadata": {
        "id": "Qysc0dR6Wpgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "XvKUpo3zsSC5",
        "outputId": "ba670f9f-6976-44b2-e4df-24538360de30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            ID                                               Body  \\\n",
              "0       chal-1  Each pack of dvds costs 76 dollars. If there i...   \n",
              "1       chal-2  Dan had $ 3 left with him after he bought a ca...   \n",
              "2       chal-3  Paco had 26 salty cookies and 17 sweet cookies...   \n",
              "3       chal-4  43 children were riding on the bus. At the bus...   \n",
              "4       chal-5  28 children were riding on the bus. At the bus...   \n",
              "..         ...                                                ...   \n",
              "995   chal-996  Paige was helping her mom plant flowers and to...   \n",
              "996   chal-997  At the zoo, a cage had 3 snakes and 75 alligat...   \n",
              "997   chal-998  Paige was helping her mom plant flowers and to...   \n",
              "998   chal-999  Mary is baking a cake. The recipe calls for 7 ...   \n",
              "999  chal-1000  The grasshopper and the frog had a jumping con...   \n",
              "\n",
              "                                              Question  \\\n",
              "0        How much do you have to pay to buy each pack?   \n",
              "1                     How much did the candy bar cost?   \n",
              "2           How many salty cookies did Paco have left?   \n",
              "3    How many children got off the bus at the bus s...   \n",
              "4    How many more children got on the bus than tho...   \n",
              "..                                                 ...   \n",
              "995                How many flower beds did they have?   \n",
              "996               How many alligators were not hiding?   \n",
              "997                    How many flowers did they grow?   \n",
              "998  How many more cups of sugar does she need to add?   \n",
              "999                         How far did the frog jump?   \n",
              "\n",
              "                       Equation  Answer             Type  \n",
              "0               ( 76.0 - 25.0 )      51      Subtraction  \n",
              "1                 ( 4.0 - 3.0 )       1      Subtraction  \n",
              "2                ( 26.0 - 9.0 )      17      Subtraction  \n",
              "3               ( 43.0 - 21.0 )      22      Subtraction  \n",
              "4               ( 30.0 - 28.0 )       2      Subtraction  \n",
              "..                          ...     ...              ...  \n",
              "995             ( 36.0 / 12.0 )       3  Common-Division  \n",
              "996             ( 75.0 - 19.0 )      56      Subtraction  \n",
              "997  ( 60.0 * ( 55.0 / 15.0 ) )     220   Multiplication  \n",
              "998               ( 7.0 - 4.0 )       3      Subtraction  \n",
              "999              ( 13.0 - 2.0 )      11      Subtraction  \n",
              "\n",
              "[1000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73ee3e9a-5724-41e2-a4b9-b99fa73b41ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Body</th>\n",
              "      <th>Question</th>\n",
              "      <th>Equation</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>chal-1</td>\n",
              "      <td>Each pack of dvds costs 76 dollars. If there i...</td>\n",
              "      <td>How much do you have to pay to buy each pack?</td>\n",
              "      <td>( 76.0 - 25.0 )</td>\n",
              "      <td>51</td>\n",
              "      <td>Subtraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>chal-2</td>\n",
              "      <td>Dan had $ 3 left with him after he bought a ca...</td>\n",
              "      <td>How much did the candy bar cost?</td>\n",
              "      <td>( 4.0 - 3.0 )</td>\n",
              "      <td>1</td>\n",
              "      <td>Subtraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chal-3</td>\n",
              "      <td>Paco had 26 salty cookies and 17 sweet cookies...</td>\n",
              "      <td>How many salty cookies did Paco have left?</td>\n",
              "      <td>( 26.0 - 9.0 )</td>\n",
              "      <td>17</td>\n",
              "      <td>Subtraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>chal-4</td>\n",
              "      <td>43 children were riding on the bus. At the bus...</td>\n",
              "      <td>How many children got off the bus at the bus s...</td>\n",
              "      <td>( 43.0 - 21.0 )</td>\n",
              "      <td>22</td>\n",
              "      <td>Subtraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>chal-5</td>\n",
              "      <td>28 children were riding on the bus. At the bus...</td>\n",
              "      <td>How many more children got on the bus than tho...</td>\n",
              "      <td>( 30.0 - 28.0 )</td>\n",
              "      <td>2</td>\n",
              "      <td>Subtraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>chal-996</td>\n",
              "      <td>Paige was helping her mom plant flowers and to...</td>\n",
              "      <td>How many flower beds did they have?</td>\n",
              "      <td>( 36.0 / 12.0 )</td>\n",
              "      <td>3</td>\n",
              "      <td>Common-Division</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>chal-997</td>\n",
              "      <td>At the zoo, a cage had 3 snakes and 75 alligat...</td>\n",
              "      <td>How many alligators were not hiding?</td>\n",
              "      <td>( 75.0 - 19.0 )</td>\n",
              "      <td>56</td>\n",
              "      <td>Subtraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>chal-998</td>\n",
              "      <td>Paige was helping her mom plant flowers and to...</td>\n",
              "      <td>How many flowers did they grow?</td>\n",
              "      <td>( 60.0 * ( 55.0 / 15.0 ) )</td>\n",
              "      <td>220</td>\n",
              "      <td>Multiplication</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>chal-999</td>\n",
              "      <td>Mary is baking a cake. The recipe calls for 7 ...</td>\n",
              "      <td>How many more cups of sugar does she need to add?</td>\n",
              "      <td>( 7.0 - 4.0 )</td>\n",
              "      <td>3</td>\n",
              "      <td>Subtraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>chal-1000</td>\n",
              "      <td>The grasshopper and the frog had a jumping con...</td>\n",
              "      <td>How far did the frog jump?</td>\n",
              "      <td>( 13.0 - 2.0 )</td>\n",
              "      <td>11</td>\n",
              "      <td>Subtraction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73ee3e9a-5724-41e2-a4b9-b99fa73b41ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-73ee3e9a-5724-41e2-a4b9-b99fa73b41ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-73ee3e9a-5724-41e2-a4b9-b99fa73b41ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mwp2 = 'mary had 38 skittles and 12 blueberries. she gave 10 skittles to anna. how many blueberries does mary have'\n",
        "ms.mwp_split(mwp2)\n"
      ],
      "metadata": {
        "id": "lxvZqB8qX81M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "#i = random.randint(0, 999)\n",
        "i = 760\n",
        "print('question number: ', i)\n",
        "split = ' ' if df['Body'][i].endswith('.') else '. '\n",
        "mwp = replace(df['Body'][i] + split + df['Question'][i])\n",
        "mwp = mwp.lower()\n",
        "print(mwp)\n",
        "print(ms.mwp_split(mwp))\n",
        "ms.statements(mwp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBgJl-aVJKNm",
        "outputId": "6756c423-94a3-4ff5-8180-bca518c76b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question number:  760\n",
            "each basket of peaches has 19 red peaches and 4 green peaches. if there are 15 such baskets. how many peaches are in 15 such baskets altogether?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  'with `validate_args=False` to turn off validation.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['each  basket   of   peaches has  19 red peaches', 'each  basket   of   peaches has  4 green peaches', 'if    there are 15  such  baskets', 'how  many  peaches are   in 15  such  baskets   altogether']\n",
            "{'baskets', 'peaches'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how  many  peaches are   in 15  such  baskets   altogether']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qtokens = nltk.word_tokenize('how much farther did the grasshopper jump than the mouse')\n",
        "tags = [pos for (word, pos) in nltk.pos_tag(qtokens)]\n",
        "tags"
      ],
      "metadata": {
        "id": "5kStcB3Ly8r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mwp = \"last week fred had 114 dollars and jason had 22 dollars. they washed cars over the weekend and now fred has 21 dollars and jason has 78 dollars. how much money did jason make over the weekend?\"\n",
        "ms.mwp_split(mwp)"
      ],
      "metadata": {
        "id": "6eB6d53oqHxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0b2e0a-5552-460c-cbee-fb1ed1c2ba47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  'with `validate_args=False` to turn off validation.')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['last  week    fred had  114 dollars',\n",
              " 'last  week    fred had  jason had 22 dollars',\n",
              " 'they washed  cars over the weekend',\n",
              " 'they washed  now fred has 21 dollars',\n",
              " 'they washed  jason has 78 dollars',\n",
              " 'how  much  money did jason   make   over   the  weekend']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eQZdRJkeAj4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}