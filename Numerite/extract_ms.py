import nltk
#nltk.download('punkt')
#nltk.download('averaged_perceptron_tagger')

import pandas as pd

import spacy
import benepar
import neuralcoref
import util

from nltk.tokenize import sent_tokenize


class MicroStatements:
  """
  return microstatemnts from input mwp
  coref resolved and conjunction resolved.
  """
  
  def __init__(self):
    """
    initialising the neuralcoref pipeline
    """
    self.nlp_pronoun = spacy.load('en_core_web_sm')
    neuralcoref.add_to_pipe(self.nlp_pronoun, greedyness=0.52)
    #benepar.download('benepar_en3')
    self.nlp = spacy.load('en_core_web_sm')
    if spacy.__version__.startswith('2'):
        self.nlp.add_pipe(benepar.BeneparComponent("benepar_en3"))
    else:
        self.nlp.add_pipe("benepar", config={"model": "benepar_en3"})


  def extract_microstatements(self, sentence):
    """
    @input : one sentence at a time without coref resolution, because coref resolution ruins the question
    @output : {set of microstatements from a single sentence input}
    
    Performed using Parsing a dependency tree generated by benepar (berkeley neural parser)
    """
    # convert all to lower case, because the logic runs on the assumption that the labels are upper case and everything else is lower
    sentence = sentence.lower()
    parts = " "
    doc = self.nlp(sentence)
    sent = list(doc.sents)[0]
    dependency_tree = sent._.parse_string
    dependency_tree = dependency_tree.replace("(", "( ")
    dependency_tree = dependency_tree.replace(")", " )")
    dependency_tree = dependency_tree.split()
    #splitting dependency tree such that each label, bracket, word is a separate elememt in the list
    i =0
    sentence_splits= [] #split sentences on the basis of presence of conjuction
    flag_complex_sentence = False #save time if the sentence is simple
    while i<len(dependency_tree):
        if dependency_tree[i] == "CC" or dependency_tree[i] == ",":
            flag_complex_sentence = True
            if parts != "":
                sentence_splits.append(parts) #if a conjuction is encountered, a new part is added to the sentence
            parts= ""
            i=i+1
        elif dependency_tree[i].islower() or dependency_tree[i].isnumeric():
            parts = parts + dependency_tree[i] + " "
            i=i+1
        else:
            i+=1
    sentence_splits.append(parts)
    if flag_complex_sentence == False:
        return [sentence]
    phrase_checker= [] #to check whether a phrase can be an independent sentence
    common_phrase = ""
    for i in sentence_splits:
        #creates a dependency tree for each part of the sentence
        doc = self.nlp(i)
        sent = list(doc.sents)[0]
        dependency_tree = sent._.parse_string
        dependency_tree = dependency_tree.replace("(", "( ")
        dependency_tree = dependency_tree.replace(")", " )")
        dependency_tree = dependency_tree.split()
        #if a verb is present in the phrase then it is an independent sentence
        if "VP" in dependency_tree:
            phrase_checker.append([True,i,dependency_tree])
            flag = False 
        else:
            phrase_checker.append([False,i])
            flag = True #tells us if even one phrase is incomplete/dependent

    if flag:
        #to find a common phrase that will complete an incomplete phrase
        for i in phrase_checker:
            if i[0]:
                last_verb_idx = max(idx for idx, val in enumerate(i[2]) if val == 'VP') #finds the last verb in an independent sentence, copies the sentence up until then
                for j in i[2][:last_verb_idx]:
                    if j.islower() or j.isnumeric():
                        common_phrase = common_phrase + j + " " #
                last_verb_idx +=2
                brackets = 1
                while brackets>0: #to include the final verb in the sentence
                    if i[2][last_verb_idx] == "(":
                        brackets+=1
                    elif i[2][last_verb_idx] == ")":
                        brackets-=1
                    elif i[2][last_verb_idx].islower() or i[2][last_verb_idx].isnumeric():
                        common_phrase = common_phrase + i[2][last_verb_idx] + " "
                    else:
                        pass
                    last_verb_idx+=1
        for i in phrase_checker:
            if not i[0]: #for all phrases that are dependent, it adds the common phrase to the start of the phrase
                i[1] = common_phrase +" "+ i[1]
    micros = []
    #appends all the found phrases to the list of microstatements
    for i in phrase_checker:
        micros.append(i[1])
    return micros

ms = MicroStatements()
inputmwp1 = "She bought 5 apples"
inputmwp2 = "If there are 2 boxes, how many pens are there in total?"
inputmwp3 = "She and I went to the supermarket"
inputmwp4 = "She drank wine and i ate fish"
inputmwp5 = "Joel went to the supermarket with Ashley. Ashley bought 5 apples and Joel ate 3 apples. How many total oranges did they buy?"
inputmwp6 = "ashley bought 5 apples and joel ate 3 oranges."
print(ms.extract_microstatements(inputmwp2))