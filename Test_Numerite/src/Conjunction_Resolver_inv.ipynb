{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_VHLRXhYvAK",
        "outputId": "53b5df46-7abb-4e24-9e62-b9a0a655c047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.1.0\n",
            "  Using cached spacy-2.1.0-cp37-cp37m-manylinux1_x86_64.whl (27.7 MB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.9)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.6)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "  Using cached preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82 kB)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "  Using cached blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2 MB)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.9.6)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "  Using cached thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.64.1)\n",
            "Installing collected packages: preshed, blis, thinc, spacy\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.8\n",
            "    Uninstalling preshed-3.0.8:\n",
            "      Successfully uninstalled preshed-3.0.8\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.1\n",
            "    Uninstalling thinc-7.4.1:\n",
            "      Successfully uninstalled thinc-7.4.1\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.3.1\n",
            "    Uninstalling spacy-2.3.1:\n",
            "      Successfully uninstalled spacy-2.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "checklist 0.0.11 requires spacy>=2.2, but you have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.2.4 preshed-2.0.1 spacy-2.1.0 thinc-7.0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_sm==2.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 5.1 MB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f7bf7b0a590>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install -U spacy==2.1.0\n",
        "!python -m spacy download en\n",
        "\n",
        "!pip install Cython --install-option=\"–no-cython-compile\"\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import spacy\n",
        "nlp_pronoun = spacy.load('en')\n",
        "import neuralcoref\n",
        "neuralcoref.add_to_pipe(nlp_pronoun,greedyness=0.52)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neuralcoref==4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YOLi9rjMTT1",
        "outputId": "5817c2e9-4ec3-4ae0-f429-36a3b9f82e42"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: neuralcoref==4.0 in /usr/local/lib/python3.7/dist-packages (4.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (1.26.19)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.23.0)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.3.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (0.9.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (4.64.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.0.9)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (7.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (3.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0) (1.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref==4.0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->neuralcoref==4.0) (3.10.0)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref==4.0) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.19 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref==4.0) (1.29.19)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref==4.0) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.19->boto3->neuralcoref==4.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.19->boto3->neuralcoref==4.0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuZYBmOLIKFr",
        "outputId": "7834c912-b42b-44df-b48d-4e579b75942d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=a1788fd66e3d6d7cdceaf0c6c05cbe16cabcf2f9667ae4d56fa50e79add7e5ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dibVrrTBcERq",
        "outputId": "af80a6e2-d072-4acf-8470-4e63ef66db9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  import nltk\n",
        "  nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2ZRjUGzLxxs",
        "outputId": "242cdd09-b924-4a32-885c-52db99d3bf51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install benepar\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46BjnbmmaDiO",
        "outputId": "72c2f902-909e-4a82-8365-95ccc8aa4fae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: benepar in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.1.97)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.13.2)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar) (1.12.1+cu113)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar) (2.1.0)\n",
            "Requirement already satisfied: transformers[tokenizers,torch]>=4.2.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (4.24.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar) (3.19.6)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (2022.6.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.7)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.10.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.9)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.9.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2022.9.24)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->benepar) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.13.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[tokenizers,torch]>=4.2.2->benepar) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[tokenizers,torch]>=4.2.2->benepar) (3.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mZE9V1fad66",
        "outputId": "3d1c1c0b-014a-4bdd-9cf5-4c7600cf5f95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_md==2.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4 MB 1.2 MB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace(sentence):\n",
        "  doc = nlp_pronoun(sentence)\n",
        "  return doc._.coref_resolved"
      ],
      "metadata": {
        "id": "1zWggYTCIV7h"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import benepar, spacy\n",
        "#benepar.download('benepar_en3')\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "if spacy.__version__.startswith('2'):\n",
        "        nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
        "else:\n",
        "        nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
      ],
      "metadata": {
        "id": "cHxvB5fPZIRF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sub_verb_object(sentence):\n",
        "    subject = \"\"\n",
        "    verb_phrase = \"\"\n",
        "    preposition_phrase = \"\"\n",
        "    #dependency_tree = sent1._.parse_string\n",
        "    doc = nlp(sentence)\n",
        "    sent = list(doc.sents)[0]\n",
        "    dependency_tree = sent._.parse_string\n",
        "    sb = 1\n",
        "    vb = 0\n",
        "    pb = 0\n",
        "\n",
        "    for i in range(len(dependency_tree)):\n",
        "      ch = dependency_tree[i]\n",
        "\n",
        "      if sb == 1:\n",
        "        if (ch.islower()==True or ch.isdigit()==True or ch==\" \" or ch==\",\" ):\n",
        "          if( (subject[:-1]!=\" \" and ch==\" \") or ch!=\" \"):\n",
        "              subject = subject+ch\n",
        "\n",
        "        \n",
        "      \n",
        "        elif ch==\"V\":\n",
        "          sb = 0\n",
        "          vb = 1\n",
        "          continue\n",
        "\n",
        "        \n",
        "      if vb == 1:\n",
        "        if (ch.islower()==True or ch.isdigit()==True or ch==\" \" or ch==\",\"):\n",
        "          if( (verb_phrase[:-1]!=\" \" and ch==\" \") or ch!=\" \"):\n",
        "            verb_phrase = verb_phrase+ch\n",
        "          \n",
        "        elif ch==\"N\" and dependency_tree[i+1] == \"P\":\n",
        "          vb = 0\n",
        "          pb = 1\n",
        "          continue\n",
        "\n",
        "      if pb == 1:\n",
        "          if (ch.islower()==True or ch.isdigit()==True or ch==\" \" or ch==\",\"):\n",
        "            if( (preposition_phrase[:-1]!=\" \" and ch==\" \") or ch!=\" \"):\n",
        "              preposition_phrase = preposition_phrase+ch\n",
        "          \n",
        "        \n",
        "         \n",
        "      \n",
        "    return {\"subject\":subject.strip(), \"verb\":verb_phrase.strip(), \"object\":preposition_phrase.strip()}"
      ],
      "metadata": {
        "id": "X5pqD-x7ZL6E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_conj(sent_list):\n",
        "\n",
        "  sub = \"\"\n",
        "  l = list()\n",
        "\n",
        "  for i in sent_list:\n",
        "    \n",
        "    if i!=\"and\":\n",
        "      \n",
        "        sub = sub+\" \"+i\n",
        "       \n",
        " \n",
        "    else:\n",
        "      if sub!=\"\":\n",
        "        l.append(sub)\n",
        "    \n",
        "        sub = \"\"\n",
        "\n",
        "  l.append(sub)\n",
        "\n",
        "\n",
        "  return l"
      ],
      "metadata": {
        "id": "yxWiUaTvZPT3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_conjunction(sentence):\n",
        "  d = extract_sub_verb_object(sentence)\n",
        "\n",
        "  subject = d[\"subject\"].replace(\",\",\"and\")\n",
        "  verb = d[\"verb\"].replace(\",\",\"and\")\n",
        "  obj = d[\"object\"].replace(\",\",\"and\")\n",
        "  subject_tokens = [i for i in subject.split(' ') if i != ' ']\n",
        "  verb_tokens = [i for i in verb.split(' ') if i!= ' ']\n",
        "  obj_tokens = [i for i in obj.split(' ') if i!=''] \n",
        "  sentences = list()\n",
        "  if \"and\" in subject_tokens:\n",
        "    split_subject = split_conj(subject_tokens)\n",
        "    \n",
        "    for i in split_subject:\n",
        "      sentences.append(i+\" \"+verb+\" \"+obj)\n",
        "    return sentences\n",
        "  if \"and\" in obj_tokens:\n",
        "    \n",
        "    split_obj = split_conj(obj_tokens)\n",
        "  \n",
        "    \n",
        "    for i in split_obj:\n",
        "      sentences.append(subject+\" \" + verb+\" \"+ i)\n",
        "  \n",
        "    return sentences\n",
        "  else:\n",
        "    sentences.append(subject+\" \"+verb+\" \"+obj)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "8LMtjeLVZR4c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "def split_mwp(mwp):\n",
        "  res = []\n",
        "  mwp_split_temp = sent_tokenize(mwp)\n",
        "  mwp_split = list()\n",
        "  for i in mwp_split_temp:\n",
        "    temp = handle_conjunction(i)\n",
        "    for j in temp:\n",
        "      mwp_split.append(j.strip())\n",
        "  for i,sent in enumerate(mwp_split):\n",
        "    if \"and\" in sent:\n",
        "      res.extend(handle_conjunction(sent))\n",
        "      continue\n",
        "    res.append(sent)\n",
        "    \n",
        "  return res"
      ],
      "metadata": {
        "id": "I9jHo2uAZYlP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_mwp(\"john and rebecca had 2 oranges.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pMqNsBLa9fI",
        "outputId": "2ea158e4-1527-44b5-81e1-ad80c601f09b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  'with `validate_args=False` to turn off validation.')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['john  had 2  oranges', 'rebecca had 2  oranges']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VifNQHO-bGtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install checklist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v470d9j4bOHT",
        "outputId": "9de63462-e174-42fe-8664-48876e1241d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting checklist\n",
            "  Downloading checklist-0.0.11.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from checklist) (1.21.6)\n",
            "Collecting spacy>=2.2\n",
            "  Downloading spacy-3.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 45.5 MB/s \n",
            "\u001b[?25hCollecting munch>=2.5\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from checklist) (0.3.6)\n",
            "Collecting jupyter>=1.0\n",
            "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist) (7.7.1)\n",
            "Requirement already satisfied: transformers>=2.8 in /usr/local/lib/python3.7/dist-packages (from checklist) (4.24.0)\n",
            "Collecting patternfork-nosql\n",
            "  Downloading patternfork_nosql-3.6.tar.gz (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 61.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist) (7.9.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist) (5.1.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist) (3.0.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist) (3.6.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist) (5.3.4)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist) (6.0.4)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist) (57.4.0)\n",
            "Collecting jedi>=0.10\n",
            "  Using cached jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist) (2.0.10)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets>=7.5->checklist) (0.8.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist) (5.7.16)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist) (6.1.0)\n",
            "Collecting qtconsole\n",
            "  Using cached qtconsole-5.4.0-py3-none-any.whl (121 kB)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist) (5.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch>=2.5->checklist) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5->checklist) (0.2.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (2.11.3)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 64.4 MB/s \n",
            "\u001b[?25hCollecting thinc<8.2.0,>=8.1.0\n",
            "  Downloading thinc-8.1.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (806 kB)\n",
            "\u001b[K     |████████████████████████████████| 806 kB 66.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (3.0.10)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "\u001b[K     |████████████████████████████████| 490 kB 66.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (0.8.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (2.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (1.10.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (4.64.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (1.0.9)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.2->checklist) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=2.2->checklist) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.2->checklist) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2->checklist) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2->checklist) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2->checklist) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2->checklist) (3.0.4)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.2->checklist) (0.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist) (0.11.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=2.2->checklist) (7.1.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist) (0.13.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist) (0.15.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist) (23.2.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist) (5.7.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist) (4.11.2)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist) (1.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.2->checklist) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist) (2.8.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist) (5.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist) (0.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist) (0.8.4)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter>=1.0->checklist) (2.6.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter>=1.0->checklist) (2.16.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist) (0.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist) (0.16.0)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/backports-csv/\u001b[0m\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist) (4.9.1)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.0 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist) (3.7)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 48.8 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 64.5 MB/s \n",
            "\u001b[?25hCollecting cheroot>=8.2.1\n",
            "  Downloading cheroot-9.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 11.6 MB/s \n",
            "\u001b[?25hCollecting portend>=2.1.1\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.8.0-py3-none-any.whl (10 kB)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist) (9.0.0)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.5.2-py3-none-any.whl (7.3 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-5.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->patternfork-nosql->checklist) (2022.6)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n",
            "Collecting autocommand\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->patternfork-nosql->checklist) (2.1.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->patternfork-nosql->checklist) (5.10.0)\n",
            "Collecting jaraco.context>=4.1\n",
            "  Downloading jaraco.context-4.2.0-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->patternfork-nosql->checklist) (1.2.0)\n",
            "Collecting cryptography>=36.0.0\n",
            "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist) (2.1.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six->patternfork-nosql->checklist) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->patternfork-nosql->checklist) (2.21)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Using cached QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
            "Building wheels for collected packages: checklist, iso-639, patternfork-nosql, python-docx, sgmllib3k\n",
            "  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165635 sha256=a77603ff1576102c6948fa68a08d3da65e4fc75f88535776fb5d4a07b527e074\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/8a/07/6446879be434879c27671c83443727d74cecf6b630c8a24d03\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169063 sha256=b12e734a07f281d41b3d4bd65b7916f2248a8db1340b89b38c699d57f00c73a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332807 sha256=5e31d3ef85fa9392e2b28c52aaf40f9ce90a3f9bf8f4c5d0a1861e2f35347093\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/72/8f/5305fe28168f93b658da9ed433b9a1d3ec90594faa0c9aaf4b\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=d2fb6c578c13f3edb7bd07a3faa82350ab63ab5a8239350104c4c6dbbf0a1913\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=1c6a80bc97b1cdebe5430fc27d0a9f732e88eb6a5fec3ad731536b6d95522135\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built checklist iso-639 patternfork-nosql python-docx sgmllib3k\n",
            "Installing collected packages: jedi, jaraco.functools, jaraco.context, autocommand, tempora, srsly, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, qtpy, preshed, portend, jaraco.collections, cryptography, cheroot, blis, thinc, qtconsole, python-docx, pdfminer.six, feedparser, cherrypy, backports.csv, spacy, patternfork-nosql, munch, jupyter, iso-639, checklist\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.6\n",
            "    Uninstalling srsly-1.0.6:\n",
            "      Successfully uninstalled srsly-1.0.6\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 2.0.1\n",
            "    Uninstalling preshed-2.0.1:\n",
            "      Successfully uninstalled preshed-2.0.1\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.2.4\n",
            "    Uninstalling blis-0.2.4:\n",
            "      Successfully uninstalled blis-0.2.4\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.0.8\n",
            "    Uninstalling thinc-7.0.8:\n",
            "      Successfully uninstalled thinc-7.0.8\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.1.0\n",
            "    Uninstalling spacy-2.1.0:\n",
            "      Successfully uninstalled spacy-2.1.0\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 blis-0.7.9 checklist-0.0.11 cheroot-9.0.0 cherrypy-18.8.0 cryptography-38.0.4 feedparser-6.0.10 iso-639-0.4.5 jaraco.classes-3.2.3 jaraco.collections-3.8.0 jaraco.context-4.2.0 jaraco.functools-3.5.2 jaraco.text-3.11.0 jedi-0.18.2 jupyter-1.0.0 munch-2.5.0 patternfork-nosql-3.6 pdfminer.six-20221105 portend-3.1.0 preshed-3.0.8 python-docx-0.8.11 qtconsole-5.4.0 qtpy-2.3.0 sgmllib3k-1.0.0 spacy-3.4.3 srsly-2.4.5 tempora-5.1.0 thinc-8.1.5 zc.lockfile-2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "blis",
                  "preshed",
                  "spacy",
                  "srsly",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "614GusTsbTEX",
        "outputId": "8b5ffab2-e19e-4621-a184-357f50162023"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_sm==2.3.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.3.1) (2.3.1)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.7)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.64.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.10.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2022.9.24)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047104 sha256=7e64c3951499371e134b34e7b7d653c66b484f7c8e5c1605fac2772ff6f4cf61\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t8ed89w9/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.1.0\n",
            "    Uninstalling en-core-web-sm-2.1.0:\n",
            "      Successfully uninstalled en-core-web-sm-2.1.0\n",
            "Successfully installed en-core-web-sm-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==2.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "m4Nd_afGNdG5",
        "outputId": "9cd022d8-a01b-43d7-fff7-b5e5b1d3ed01"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.3.1\n",
            "  Using cached spacy-2.3.1-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.1) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.1) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.1) (4.64.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.1) (2.0.7)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Using cached blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.1) (1.0.6)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Using cached preshed-3.0.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
            "Collecting thinc==7.4.1\n",
            "  Using cached thinc-7.4.1-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.1) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.1) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.1) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.1) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.1) (2022.9.24)\n",
            "Installing collected packages: preshed, blis, thinc, spacy\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 2.0.1\n",
            "    Uninstalling preshed-2.0.1:\n",
            "      Successfully uninstalled preshed-2.0.1\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.2.4\n",
            "    Uninstalling blis-0.2.4:\n",
            "      Successfully uninstalled blis-0.2.4\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.0.8\n",
            "    Uninstalling thinc-7.0.8:\n",
            "      Successfully uninstalled thinc-7.0.8\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.1.0\n",
            "    Uninstalling spacy-2.1.0:\n",
            "      Successfully uninstalled spacy-2.1.0\n",
            "Successfully installed blis-0.4.1 preshed-3.0.8 spacy-2.3.1 thinc-7.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "blis",
                  "preshed",
                  "spacy",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import checklist\n",
        "from checklist.editor import Editor\n",
        "\n",
        "editor = Editor()\n",
        "\n",
        "sample_test_data = editor.template(\n",
        "    \"{mask} and {mask} had 2 {mask} and 6 {mask}.\", pos=[\"good\", \"great\"], nsamples=10\n",
        ").data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6KgyddCbuWH",
        "outputId": "d2416367-96dc-4e9e-f428-d5db3ab02c17"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/checklist/text_generation.py:171: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  to_pred = torch.tensor(to_pred, device=self.device).to(torch.int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDNhzDi4dGgO",
        "outputId": "35e4e7e4-0b49-4fb0-9de0-490d9c88542c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Peter and Mary had 2 sons and 6 daughters.',\n",
              " 'William and Kate had 2 sons and 6 daughters.',\n",
              " 'William and Elizabeth had 2 boys and 6 girls.',\n",
              " 'William and Mary had 2 daughters and 6 sons.',\n",
              " 'Dad and Mom had 2 children and 6 grandchildren.',\n",
              " 'Mom and Dad had 2 boys and 6 girls.',\n",
              " 'William and Mary had 2 sons and 6 children.',\n",
              " 'William and Elizabeth had 2 sons and 6 grandchildren.',\n",
              " 'Mom and Dad had 2 brothers and 6 sisters.',\n",
              " 'Mary and Joseph had 2 sons and 6 daughters.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sample_test_data:\n",
        "  print(split_mwp(i.lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6eoNIsPdrzS",
        "outputId": "d08556fe-48f1-499c-9bc2-017d657d3778"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['peter had  2 sons', 'peter had  6 daughters', 'mary had  2 sons', 'mary had  6 daughters']\n",
            "['william had  2 sons', 'william had  6 daughters', 'kate had  2 sons', 'kate had  6 daughters']\n",
            "['william had  2 boys', 'william had  6 girls', 'elizabeth had  2 boys', 'elizabeth had  6 girls']\n",
            "['william had  2 daughters', 'william had  6 sons', 'mary had  2 daughters', 'mary had  6 sons']\n",
            "['dad had  2 children', 'dad had  6 grandchildren', 'mom had  2 children', 'mom had  6 grandchildren']\n",
            "['mom had  2 boys', 'mom had  6 girls', 'dad had  2 boys', 'dad had  6 girls']\n",
            "['william had  2 sons', 'william had  6 children', 'mary had  2 sons', 'mary had  6 children']\n",
            "['william had  2 sons', 'william had  6 grandchildren', 'elizabeth had  2 sons', 'elizabeth had  6 grandchildren']\n",
            "['mom had  2 brothers', 'mom had  6 sisters', 'dad had  2 brothers', 'dad had  6 sisters']\n",
            "['mary had  2 sons', 'mary had  6 daughters', 'joseph had  2 sons', 'joseph had  6 daughters']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_test_data_1 = editor.template(\n",
        "    \"there are 10 {mask} and 20 {mask} in Philip's collection.\", nsamples=10\n",
        ").data"
      ],
      "metadata": {
        "id": "d24gedWsbvv4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_test_data_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGnK4I51g7MR",
        "outputId": "4d03bb64-a1e5-4bc2-a6fa-513bdcfe6c21"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"there are 10 CDs and 20 tapes in Philip's collection.\",\n",
              " \"there are 10 books and 20 DVDs in Philip's collection.\",\n",
              " \"there are 10 albums and 20 films in Philip's collection.\",\n",
              " \"there are 10 books and 20 CDs in Philip's collection.\",\n",
              " \"there are 10 drawings and 20 photographs in Philip's collection.\",\n",
              " \"there are 10 albums and 20 singles in Philip's collection.\",\n",
              " \"there are 10 cats and 20 birds in Philip's collection.\",\n",
              " \"there are 10 letters and 20 notes in Philip's collection.\",\n",
              " \"there are 10 poems and 20 essays in Philip's collection.\",\n",
              " \"there are 10 CDs and 20 DVDs in Philip's collection.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sample_test_data_1:\n",
        "  print(split_mwp(i.lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8fP6lW4g_bt",
        "outputId": "00711c7b-92bb-4772-f289-ae780b464449"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['there are  10 cds', 'there are  20 tapes in philip s collection']\n",
            "['there are  10 books', 'there are  20 dvds in philip s collection']\n",
            "['there are  10 albums', 'there are  20 films in philip s collection']\n",
            "['there are  10 books', 'there are  20 cds in philip s collection']\n",
            "['there are  10 drawings', 'there are  20 photographs in philip s collection']\n",
            "['there are  10 albums', 'there are  20 singles in philip s collection']\n",
            "['there are  10 cats', 'there are  20 birds in philip s collection']\n",
            "['there are  10 letters', 'there are  20 notes in philip s collection']\n",
            "['there are  10 poems', 'there are  20 essays in philip s collection']\n",
            "['there are  10 cds', 'there are  20 dvds in philip s collection']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_test_data_2 = editor.template(\n",
        "    \"{mask} received 10 {mask} and 3 {mask} and 4 {mask} in his {mask}\", nsamples=10\n",
        ").data\n",
        "sample_test_data_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kTeWlbahKOY",
        "outputId": "b826c013-35b8-43d0-af92-dfa91a778228"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Williams received 10 points and 3 rebounds and 4 blocks in his debut',\n",
              " 'Brown received 10 starts and 3 TDs and 4 interceptions in his career',\n",
              " 'Walker received 10 tackles and 3 sacks and 4 interceptions in his debut',\n",
              " 'Johnson received 10 points and 3 rebounds and 4 assists in his debut',\n",
              " 'Williams received 10 points and 3 rebounds and 4 steals in his debut',\n",
              " 'Brown received 10 starts and 3 touchdowns and 4 interceptions in his career',\n",
              " 'Bush received 10 percent and 3 percent and 4 percent in his primary',\n",
              " 'Wilson received 10 tackles and 3 sacks and 4 interceptions in his debut',\n",
              " 'Brown received 10 starts and 3 starts and 4 losses in his career',\n",
              " 'Hart received 10 starts and 3 wins and 4 losses in his career']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_microstatements(sentence):\n",
        "        \"\"\"\n",
        "        @input : one sentence at a time without coref resolution, because coref resolution ruins the question\n",
        "        @output : {set of microstatements from a single sentence input}\n",
        "        Performed using Parsing a dependency tree generated by benepar (berkeley neural parser)\n",
        "        \"\"\"\n",
        "        # convert all to lower case, because the logic runs on the assumption that the labels are upper case and everything else is lower\n",
        "        sentence = sentence.lower()\n",
        "        parts = \" \"\n",
        "        doc = nlp(sentence)\n",
        "        try:\n",
        "            #print(list(doc.sents))\n",
        "            sent = list(doc.sents)[0]\n",
        "        except:\n",
        "            return []\n",
        "        dependency_tree = sent._.parse_string\n",
        "        #print(dependency_tree)\n",
        "        dependency_tree = dependency_tree.replace(\"(\", \"( \")\n",
        "        dependency_tree = dependency_tree.replace(\")\", \" )\")\n",
        "        dependency_tree = dependency_tree.split()\n",
        "        #splitting dependency tree such that each label, bracket, word is a separate elememt in the list\n",
        "        i =0\n",
        "        sentence_splits= [] #split sentences on the basis of presence of conjuction\n",
        "        flag_complex_sentence = False #save time if the sentence is simple\n",
        "        while i<len(dependency_tree):\n",
        "            if dependency_tree[i] == \"CC\" or (dependency_tree[i] == \",\" and dependency_tree[i+1] == ','):\n",
        "                flag_complex_sentence = True\n",
        "                if parts != \"\":\n",
        "                    sentence_splits.append(parts) #if a conjuction is encountered, a new part is added to the sentence\n",
        "                parts= \"\"\n",
        "                while(dependency_tree[i] != \")\"):\n",
        "                    i+=1\n",
        "            elif dependency_tree[i].islower() or dependency_tree[i].isnumeric():\n",
        "                parts = parts + dependency_tree[i] + \" \"\n",
        "                i=i+1\n",
        "            else:\n",
        "                i+=1\n",
        "        sentence_splits.append(parts)\n",
        "        #print(sentence_splits)\n",
        "        if flag_complex_sentence == False:\n",
        "            return [sentence]\n",
        "        phrase_checker= [] #to check whether a phrase can be an independent sentence\n",
        "        #extract_noun = []\n",
        "        flag_no_verb = False\n",
        "        for i in sentence_splits:\n",
        "            #creates a dependency tree for each part of the sentence\n",
        "            doc = nlp(i)\n",
        "            sent = list(doc.sents)[0]\n",
        "            #print(sent)\n",
        "            dependency_tree = sent._.parse_string\n",
        "            dependency_tree = dependency_tree.replace(\"(\", \"( \")\n",
        "            dependency_tree = dependency_tree.replace(\")\", \" )\")\n",
        "            dependency_tree = dependency_tree.split()\n",
        "            #print(dependency_tree)\n",
        "            #if a verb is present in the phrase then it is an independent sentence\n",
        "            if \"VP\" in dependency_tree:\n",
        "                phrase_checker.append([True,i,dependency_tree])\n",
        "                #flag_no_verb = False \n",
        "            else:\n",
        "                phrase_checker.append([False,i])\n",
        "                #print(i, dependency_tree)\n",
        "                flag_no_verb = True\n",
        "                break\n",
        "            #tells us if even one phrase is incomplete/dependent\n",
        "\n",
        "            # if \"JJ\" in dependency_tree:\n",
        "            #     last_adj_idx = max(idx for idx, val in enumerate(dependency_tree) if val == 'JJ')\n",
        "            #     j = last_adj_idx\n",
        "            #     flag_adj = False\n",
        "            #     while(j<len(dependency_tree)):\n",
        "            #         if dependency_tree[j] == \"NN\" or dependency_tree[j] == \"NNS\" or dependency_tree[j] == \"NNP\" or dependency_tree[j] == \"NNPS\":\n",
        "            #             extract_noun.append([i, dependency_tree[j+1]])\n",
        "            #             flag_adj = True\n",
        "            #         j+=1\n",
        "            #     if flag_adj == False:\n",
        "            #         extract_noun.append([i, \"\"])\n",
        "        \n",
        "                    \n",
        "\n",
        "        #print(extract_noun)\n",
        "        if flag_no_verb:\n",
        "            #print(\"never here\")\n",
        "            common_phrases_ls = []\n",
        "            #to find a common phrase that will complete an incomplete phrase\n",
        "            for i in phrase_checker:\n",
        "                if i[0]:\n",
        "                    #print(i[1])\n",
        "                    common_phrase = \"\"\n",
        "                    last_verb_idx = max(idx for idx, val in enumerate(i[2]) if val == 'VP') #finds the last verb in an independent sentence, copies the sentence up until then\n",
        "                    for j in i[2][:last_verb_idx]:\n",
        "                        if j.islower() or j.isnumeric():\n",
        "                            common_phrase = common_phrase + j + \" \" #\n",
        "                    last_verb_idx +=2\n",
        "                    brackets = 1\n",
        "                    while brackets>0: #to include the final verb in the sentence\n",
        "                        if i[2][last_verb_idx] == \"(\":\n",
        "                            brackets+=1\n",
        "                        elif i[2][last_verb_idx] == \")\":\n",
        "                            brackets-=1\n",
        "                        elif i[2][last_verb_idx].islower() or i[2][last_verb_idx].isnumeric():\n",
        "                            common_phrase = common_phrase + i[2][last_verb_idx] + \" \"\n",
        "                        else:\n",
        "                            pass\n",
        "                        last_verb_idx+=1\n",
        "                    common_phrases_ls.append(common_phrase)\n",
        "            true_count = 0 #checks which common phrase is applicable to which incomplete phrase\n",
        "            for i in phrase_checker: \n",
        "                if i[0]:\n",
        "                    true_count+=1 \n",
        "                if not i[0]: #for all phrases that are dependent, it adds the common phrase to the start of the phrase\n",
        "                    try:\n",
        "                        i[1] = common_phrases_ls[true_count-1] +\" \"+ i[1]\n",
        "                    except:\n",
        "                        i[1] = i[1]\n",
        "\n",
        "        micros = []\n",
        "        #appends all the found phrases to the list of microstatements\n",
        "        for i in phrase_checker:\n",
        "            micros.append(i[1])\n",
        "        #print(micros)\n",
        "        '''in case of a noun that proceeds an adjective, the noun does not get included into the microstatement.\n",
        "        This is to append that extra noun in all sentences where it remains missing'''\n",
        "        #print(micros)\n",
        "        extract_noun = []\n",
        "        for i in micros:\n",
        "           # print(i)\n",
        "            doc = nlp(i)\n",
        "            sent = list(doc.sents)[0]\n",
        "            dependency_tree = sent._.parse_string\n",
        "            dependency_tree = dependency_tree.replace(\"(\", \"( \")\n",
        "            dependency_tree = dependency_tree.replace(\")\", \" )\")\n",
        "            dependency_tree = dependency_tree.split()\n",
        "            #print(dependency_tree)\n",
        "            if \"JJ\" in dependency_tree:\n",
        "                #print(dependency_tree) #checks to see if this is a concern for us at all\n",
        "                last_adj_idx = max(idx for idx, val in enumerate(dependency_tree) if val == 'JJ')\n",
        "                j = last_adj_idx #finds the last adjective in the sentence\n",
        "                #print(j)\n",
        "                flag_adj = False\n",
        "                temp_noun = \"\"\n",
        "                while(j<len(dependency_tree)): #checks to see if there is a noun after the adjective\n",
        "                    if dependency_tree[j] == \"NN\" or dependency_tree[j] == \"NNS\" or dependency_tree[j] == \"NNP\" or dependency_tree[j] == \"NNPS\":\n",
        "                        temp_noun = temp_noun + dependency_tree[j+1] + \" \"\n",
        "                        flag_adj = True\n",
        "                    j+=1\n",
        "                extract_noun.append([i,temp_noun])   \n",
        "                \n",
        "                if flag_adj == False:\n",
        "                    extract_noun.append([i, \"\"]) #if there is no noun after the adjective, then the noun is missing and needs to be added\n",
        "            else:\n",
        "                extract_noun.append([i,\"ignore\"]) #if there is no adjective, then we don't need to worry about this sentence\n",
        "        modified_micros = []\n",
        "        #print(extract_noun)\n",
        "        for i in range(len(extract_noun)):\n",
        "            #print(\"extract_nouns\", extract_noun[i])\n",
        "            if extract_noun[i][1] !=\"\": #if the noun is not empty, then we add it to the microstatements list, since it is a complete sentence\n",
        "                #print(\"i was here\")\n",
        "                modified_micros.append(extract_noun[i][0])\n",
        "            else:\n",
        "                #print(\"i came here\")\n",
        "                temp = i+1\n",
        "                while(temp<len(extract_noun)):\n",
        "                    if extract_noun[temp][1] ==\"ignore\": #sentences that don't have an adjective are ignored\n",
        "                        temp+=1\n",
        "                        continue\n",
        "                    elif extract_noun[temp][1]==\"\": #if the next sentence also doesn't have a noun, then we ignore it\n",
        "                        temp+=1\n",
        "                        continue\n",
        "                    else: #if the sentence has a noun, we extract that and add it to the current microstatement\n",
        "                        temp_micro = extract_noun[i][0] + \" \" + extract_noun[temp][1]\n",
        "                        modified_micros.append(temp_micro)\n",
        "                        break\n",
        "                    temp+=1\n",
        "        return modified_micros"
      ],
      "metadata": {
        "id": "ktk2WwPTrrt9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from word2number import w2n\n",
        "\n",
        "    \n",
        "def convertNumberNames(sentence):\n",
        "    lstr = re.findall( r'\\w+|[^\\s\\w]+', sentence)\n",
        "    # print(lstr)\n",
        "    res = []\n",
        "    i = 0\n",
        "    status = []\n",
        "    # False if word isnt a number indicated by exception\n",
        "    for i in range(len(lstr)):\n",
        "        try:\n",
        "            w2n.word_to_num(lstr[i])\n",
        "            status.append(True)\n",
        "        except:\n",
        "            # for complex number names like \"one hundred and thirty five\"\n",
        "            if lstr[i]=='and' and i>0 and status[i-1] == True:\n",
        "                if i<len(lstr)-1:\n",
        "                    try:\n",
        "                        w2n.word_to_num(lstr[i+1])\n",
        "                        status.append(True)\n",
        "                    except:\n",
        "                        status.append(False)\n",
        "                else:\n",
        "                    status.append(False)\n",
        "            else:\n",
        "                status.append(False)\n",
        "    \n",
        "    j = 0\n",
        "    # print(status)\n",
        "    # convert all the consecutive True's to a number eg. ten -> 10; thirty, five -> 35\n",
        "    final_ls = []\n",
        "    while j<len(lstr):\n",
        "        quant = \"\"\n",
        "        while(status[j]==True):\n",
        "            quant = quant+lstr[j]+\" \"\n",
        "            j=j+1\n",
        "        if quant!=\"\":\n",
        "            try:\n",
        "                final_ls.extend([str(w2n.word_to_num(quant)),lstr[j]])\n",
        "            except:\n",
        "                final_ls.extend([quant.strip(),lstr[j]])\n",
        "        else:\n",
        "            final_ls.append(lstr[j])\n",
        "        j=j+1\n",
        "    final_ls = \" \".join(final_ls)\n",
        "    return final_ls\n",
        "\n",
        "def appendQuantities(txt):\n",
        "    wordsList = nltk.word_tokenize(txt)\n",
        "    #wordsList = [w for w in wordsList if not w in stop_words]\n",
        "    \n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        "    # determining the units in the word problem \n",
        "    \n",
        "    unit = []\n",
        "    for i in range(len(tagged)-2):\n",
        "        #print(tagged[i])\n",
        "        if tagged[i][1] == \"CD\":\n",
        "            #print(tagged[i], \"Here\")\n",
        "            j = i+1\n",
        "            while(j<len(tagged)-1):\n",
        "                #print(tagged[j])\n",
        "                if (tagged[j][1]==\"NN\" or tagged[j][1]==\"NNS\"):\n",
        "                    #print(tagged[i][0],\"HERE\")\n",
        "                    unit.append([tagged[j][0], j])\n",
        "                    #print(\"Unit\", unit)\n",
        "                    break\n",
        "                else:\n",
        "                    j=j+1\n",
        "                    break\n",
        "    #appending units\n",
        "    #print(unit) \n",
        "    unit_count = 0\n",
        "    new_mwp = \"\"\n",
        "    for i in range(len(tagged)):\n",
        "        if unit_count<len(unit)-1 and unit[unit_count][1] == i :\n",
        "            unit_count+=1\n",
        "        if (tagged[i][1] == 'CD' and tagged[i+1][1]!='NN' and tagged[i+1][1]!='NNS'):\n",
        "                #tagged.insert(i+1,unit[unit_count])\n",
        "                new_mwp = new_mwp + tagged[i][0] + \" \" + unit[unit_count][0] + \" \"\n",
        "        else:\n",
        "            new_mwp = new_mwp + tagged[i][0] + \" \"\n",
        "    \n",
        "    return new_mwp\n",
        "\n",
        "    # entities = list()\n",
        "    # s =\"\"\n",
        "    # #extracting quantities\n",
        "    # for i in range(len(tagged)-2):\n",
        "    #     if tagged[i][1] == \"CD\":\n",
        "    #         #if tagged[i+1][1] == \"NN\" or tagged[i+1][1]=='NNP'or tagged[i+1][1]=='NNPS' or tagged[i+2][1]=='NN' or tagged[i+2][1]=='NNP'or tagged[i+2][1]=='NNPS' or  tagged[i+1][1]=='NNS' or tagged[i+2][1]=='NNS' :\n",
        "    #         if tagged[i+1][1] == \"NN\" or tagged[i+2][1]=='NN' or tagged[i+1][1]=='NNS' or tagged[i+2][1]=='NNS' :\n",
        "    #             s = tagged[i][0] + \" \"+ tagged[i+1][0]\n",
        "    #             entities.append(s)\n",
        "    \n",
        "    # return(entities)\n",
        "#print(convertNumberNames(\"Joe has five and amy has 6 apples\"))"
      ],
      "metadata": {
        "id": "darGK5hYH7Pm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_question(microstatements):\n",
        "    # microstatements = mwp.split(\".\") ##microstatements module fits here, for now i have simply split sentence at full stop\n",
        "    statements = []\n",
        "    question = []\n",
        "    question_words = [\"evaluate\", \"calculate\", \"find\", \"determine\",\"how\", \"what\",\"identify\"]\n",
        "    for i in microstatements:\n",
        "        flag_q = False\n",
        "        for j in question_words:\n",
        "            if j in i:\n",
        "                question.append(i)\n",
        "                flag_q = True\n",
        "                break\n",
        "        if flag_q == False:\n",
        "            statements.append(i)\n",
        "\n",
        "    if len(question)==0:\n",
        "        question.append(microstatements[-1])\n",
        "        statements = statements[:len(statements)-1]\n",
        "    # question = microstatements[-1]\n",
        "    # statements = microstatements[:len(microstatements)-1]\n",
        "    # for i in microstatements:\n",
        "    #     if isQuestion_DL(i):\n",
        "    #         question.append(i)\n",
        "    #     else:\n",
        "    #         statements.append(i)  \n",
        "    return {\"question\":question,\"statements\":statements}"
      ],
      "metadata": {
        "id": "UQ_XLqpJLhUH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_mwp(mwp):\n",
        "        \"\"\"\n",
        "\t\n",
        "\t\tconverts to Lowercase\n",
        "\t\tchanges number names to numbers\n",
        "\t\t\"\"\"\n",
        "        mwp =  mwp.lower()\n",
        "        #mwp = spelling_correction(mwp)\n",
        "        mwp = convertNumberNames(mwp)\n",
        "        mwp = appendQuantities(mwp)\n",
        "        #print(mwp)\n",
        "        return mwp\n",
        "    \n",
        "def resolve_coref(sentence):\n",
        "        \"\"\" \n",
        "\t\tNeural coref resolution function\n",
        "\t\t@input : sentence without coref resolution\n",
        "\t\t@output : coref resolved sentence \n",
        "\t\t\"\"\"\n",
        "        doc = nlp_pronoun(sentence)\n",
        "        #self.mwp = doc._.coref_resolved\n",
        "        return doc._.coref_resolved\n",
        "def get_microstatements(mwp):\n",
        "        micros = []\n",
        "        mwp = clean_mwp(mwp)\n",
        "        #self.resolve_coref()\n",
        "        for sent in mwp.split('.'):\n",
        "            micros.extend(extract_microstatements(sent))\n",
        "            # print('For sentence:\\n', sent, '\\nMS:\\n', micros)\n",
        "        final_micros = []\n",
        "        for i in micros:\n",
        "            i = re.sub(r'[^\\w\\s]', '', i)\n",
        "            final_micros.append(i)\n",
        "        quesornot = identify_question(final_micros)\n",
        "        #print(quesornot)\n",
        "        body_string = \"\"\n",
        "        for i in quesornot['statements']:\n",
        "            body_string = body_string + i + \".\"\n",
        "        print('---------before neuralcoref------------------')\n",
        "        #body_string = resolve_coref(body_string)\n",
        "        final_micros = body_string[:-1].split(\".\")\n",
        "        final_micros.append(quesornot['question'][0])\n",
        "        #print(final_micros)\n",
        "        return final_micros"
      ],
      "metadata": {
        "id": "_49elr13KT8z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sample_test_data:\n",
        "  print(get_microstatements(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiXFMB5YLl-d",
        "outputId": "e019dfb6-5014-48fd-fa97-e47ed9c373b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------before neuralcoref------------------\n",
            "[' peter ', ' ']\n",
            "---------before neuralcoref------------------\n",
            "[' william ', ' ']\n",
            "---------before neuralcoref------------------\n",
            "[' william ', ' ']\n",
            "---------before neuralcoref------------------\n",
            "[' william ', ' ']\n",
            "---------before neuralcoref------------------\n",
            "[' dad ', ' ']\n",
            "---------before neuralcoref------------------\n",
            "[' mom ', ' ']\n",
            "---------before neuralcoref------------------\n",
            "[' william ', ' ']\n",
            "---------before neuralcoref------------------\n",
            "[' william ', ' ']\n",
            "---------before neuralcoref------------------\n",
            "[' mom ', ' ']\n",
            "---------before neuralcoref------------------\n",
            "[' mary ', ' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sample_test_data:\n",
        "  print(i)\n",
        "  print(extract_microstatements(i.lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xelxyF1vsSGC",
        "outputId": "7c3f814e-b325-432a-cce9-805d8647db2c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peter and Mary had 2 sons and 6 daughters.\n",
            "[' peter ']\n",
            "William and Kate had 2 sons and 6 daughters.\n",
            "[' william ']\n",
            "William and Elizabeth had 2 boys and 6 girls.\n",
            "[' william ']\n",
            "William and Mary had 2 daughters and 6 sons.\n",
            "[' william ']\n",
            "Dad and Mom had 2 children and 6 grandchildren.\n",
            "[' dad ']\n",
            "Mom and Dad had 2 boys and 6 girls.\n",
            "[' mom ']\n",
            "William and Mary had 2 sons and 6 children.\n",
            "[' william ']\n",
            "William and Elizabeth had 2 sons and 6 grandchildren.\n",
            "[' william ']\n",
            "Mom and Dad had 2 brothers and 6 sisters.\n",
            "[' mom ']\n",
            "Mary and Joseph had 2 sons and 6 daughters.\n",
            "[' mary ']\n"
          ]
        }
      ]
    }
  ]
}